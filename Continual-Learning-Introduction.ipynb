{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install avalanche-lib==0.3.1 jupyterlab==4.0.2 pandas==2.0.2 seaborn==0.12.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning \n",
    "## Introduction and fundamental techniques\n",
    "Author: Mateusz WÃ³jcik  \n",
    "Contact: mateusz.wojcik@pwr.edu.pl\n",
    "\n",
    "This notebook contains introduction into Continual Learning in form of a interactive hands-on playground. The aim was to provide a Python environment where You will be able to dive into the Continual Learning world by preparing and running some practical experiments. We have prepared ready to run experimental setups covering various Continual Learning scenarios.\n",
    "\n",
    "Don't hesitate to make some mess by chaning setups, models, scenarios, methods and other.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Continual Learning?\n",
    "\n",
    "Continual Learning studies the problem of learning from an infinite stream of data, with the goal of gradually extending acquired knowledge and using it for future learning [1]. So in a Continual Learning scenario:\n",
    "- we don't have all of the data at the same time\n",
    "- model is trained on the stream of data\n",
    "- data domain may change\n",
    "- class set may change (in the classification problems)\n",
    "\n",
    "Across the scientific literature, Continual Learning is also called: Incremental Learning, Sequential Learning, Continuous Learning and Lifelong Learning.\n",
    "\n",
    "![CL-definition.png](https://drive.google.com/uc?export=view&id=1ralMCUbR6EeL8h37UWYy6Jkh0oBAItUt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To ensure a quality and reproducibility of experiments we have used the Avalanche [2]. Avalanche is a modern, rich featured Continual Learning library which takes take about a lot of bolierplate code, provides access to many useful datasets and delivers implementation of common Continual Learning methods.\n",
    "\n",
    "You can find more information in the [Avalanche Github](https://github.com/ContinualAI/avalanche \"Avalanche Github\")\n",
    "\n",
    "![avalanche.png](https://drive.google.com/uc?export=view&id=11OWjDlihic69yLiyA83FXnz2Qpv0HuBx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks import SplitMNIST, PermutedMNIST, RotatedMNIST, ni_benchmark\n",
    "from avalanche.benchmarks.datasets import MNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import EWC, GEM, Naive, JointTraining, Replay\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common setup across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './tmp/'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_TEST = 256\n",
    "SEED = 1\n",
    "\n",
    "TEST_ACCURACY = 'Top1_Acc_Stream/eval_phase/test_stream/Task000'\n",
    "DEFAULT_EVALUATOR = EvaluationPlugin(\n",
    "    accuracy_metrics(\n",
    "        minibatch=False, epoch=False, experience=True, stream=True\n",
    "    ),\n",
    "    loss_metrics(minibatch=False, epoch=False, experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    ")\n",
    "NO_LOGGING_EVALUATOR = EvaluationPlugin(\n",
    "    accuracy_metrics(\n",
    "        minibatch=False, epoch=False, experience=True, stream=True\n",
    "    ),\n",
    "    loss_metrics(minibatch=False, epoch=False, experience=True, stream=True),\n",
    "    loggers=[],\n",
    ")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "To not make thing overly complicated at the beggining, we prepared simple neural network (we called it MLP to keep namespace clear). The priority is to make training easy and quick, so we will be able to perform more experiments.\n",
    "\n",
    "Keep in mind, that You are welcome with modyfying this architecture by changing/adding layers, modifying activation functions and experimenting with the dropout hyperparameter. But for the first notebook pass we suggest to keep this model intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_layer_size: int = 784, hidden_layer_size: int = 256, output_layer_size: int = 10, drop_rate: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self._input_layer_size = input_layer_size\n",
    "        self._layers = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=input_layer_size,\n",
    "                out_features=hidden_layer_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=drop_rate),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_layer_size,\n",
    "                out_features=output_layer_size,\n",
    "            ),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.contiguous()\n",
    "        x = x.view(x.size(0), self._input_layer_size)\n",
    "        return self._layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline learning (classic)\n",
    "\n",
    "Offline learning is the classic procedure that probably everybody has in mind when hear \"model training\". All the data is available at once, we have train (+ validation) and test sets and after training is finished, no chages are made in model parameters.\n",
    "\n",
    "Let's train defined model in the offline regime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline training using Pytorch\n",
    "\n",
    "Firstly, we will train a model using standard procedure in Pytorch library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use MNIST dataset to make training and evaluation quick and smooth. It is well known and the most often used in the Continual Learning literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=DATA_PATH,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=mnist_train,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=mnist_test,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & test loops\n",
    "\n",
    "Just a standard code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, epoch: int, optimizer: torch.optim.Optimizer) -> None:\n",
    "    loss = CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_loss = loss(output, target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 250 == 0:\n",
    "          print(\n",
    "            f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {train_loss.item():.6f}'\n",
    "          )\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader) -> None:\n",
    "    loss = CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    preds = torch.tensor([]).to(DEVICE)\n",
    "    expected = torch.tensor([]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += loss(output, target)\n",
    "            \n",
    "            pred = output.max(1)[1]\n",
    "            preds = torch.cat((preds, pred))\n",
    "            expected = torch.cat((expected, target))\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = Accuracy(task=\"multiclass\", num_classes=10).to(DEVICE)\n",
    "    \n",
    "    print(f'Test Accuracy: {acc(preds, expected).item():.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation\n",
    "\n",
    "Here is the experiment running code. Let's run the cell below and check what accuracy we will achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 3\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "DROP_RATE = 0.5\n",
    "\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE).to(DEVICE)\n",
    "optimizer = Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(epoch=epoch, model=model, train_loader=train_loader, optimizer=optimizer)\n",
    "    test(model=model, test_loader=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline training using Avalanche\n",
    "\n",
    "Was the code above a little bit complicated, wasn't it? \n",
    "\n",
    "Classic training procedure still has a lot of boilerplate code that can be \"hidden\". We can use Pytorch Lightning to create nice looking training code, but we can also use Avalanche library to perform classic, offline training. Let's do the same as in the \"Offline training\" section, but using the Avalanche library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 3\n",
    "DROP_RATE = 0.5\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "\n",
    "# Model.\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Dataset\n",
    "mnist = SplitMNIST(n_experiences=1)\n",
    "train_stream = mnist.train_stream\n",
    "test_stream = mnist.test_stream\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Joint training strategy\n",
    "joint_train = JointTraining(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=DEFAULT_EVALUATOR,\n",
    ")\n",
    "\n",
    "# Train and test\n",
    "results = []\n",
    "joint_train.train(train_stream)\n",
    "evaluation_result = joint_train.eval(test_stream)\n",
    "results.append(evaluation_result)\n",
    "\n",
    "pprint(evaluation_result)\n",
    "print(f\"Final accuracy: {evaluation_result[TEST_ACCURACY] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to achieve similar accuracy by writing less code and simplify training procedure. Metric that is our iterest is 'Top1_Acc_Stream/eval_phase/test_stream/Task000', which means just an Accurracy over all classes. Why such a complicated name?\n",
    "- **Top1_Acc_Stream** - we calculate Accuracy in the most common Top 1 setup (the most probable class only) and it is the overall result on the whole data in the test stream (test dataset)\n",
    "- **eval_phase/test_stream** - evaluation performed on the test stream\n",
    "- **Task000** - identifies the task, since here we have just one task this is quite redundant. But it's necessary when we have more that one task (e.g. Task Incremental Scenario).\n",
    "\n",
    "For the further experiments we also need metric in the unified format to make its comparison and visualization easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning\n",
    "\n",
    "An important hyperparameter here is \"N_EXPERIENCES\" (n_experiences). It defined on how many \"portions\" the data will be divided and can be any positive number lower that train sieze. For example\n",
    "- 1 means all the data will be available at once\n",
    "- 10 means data will be splitted into 10 parts\n",
    "- len(train_size) would mean each incoming portion of data contains only one example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive approach\n",
    "\n",
    "The simplest Continual Learning approach is to train model incrementally in the same way as we would train offline. The \"Naive\" approach assumes training the model on every incoming data batch without any additional actions. So we choose hyperparameters fo the model and train on first batch, then on the second batch and so on and so forth.\n",
    "\n",
    "![Naive-method.png](https://drive.google.com/uc?export=view&id=1elj-Aw760O1a-OiAEEc_dRyep5HVQYP3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 1\n",
    "DROP_RATE = 0.5\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10 \n",
    "\n",
    "# Model\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "def train_incremental(n_experiences: int, evaluator: EvaluationPlugin = DEFAULT_EVALUATOR) -> list[dict[str, float]]:\n",
    "    # Dataset\n",
    "    mnist = ni_benchmark(\n",
    "        mnist_train,\n",
    "        mnist_test,\n",
    "        n_experiences=n_experiences,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "        balance_experiences=True\n",
    "    )\n",
    "    \n",
    "    train_stream = mnist.train_stream\n",
    "    test_stream = mnist.test_stream\n",
    "    \n",
    "    # Prepare for training & testing\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    # Joint training strategy\n",
    "    naive_train = Naive(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        train_epochs=TRAIN_EPOCHS,\n",
    "        train_mb_size=BATCH_SIZE_TRAIN,\n",
    "        eval_mb_size=BATCH_SIZE_TEST,\n",
    "        device=DEVICE,\n",
    "        evaluator=evaluator,\n",
    "    )\n",
    "    \n",
    "    # Train and test\n",
    "    results = []\n",
    "    naive_train.train(train_stream)\n",
    "    evaluation_result = naive_train.eval(test_stream)\n",
    "    results.append(evaluation_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "N_EXPERIENCES = 10 \n",
    "\n",
    "evaluation_result = train_incremental(n_experiences=N_EXPERIENCES)\n",
    "\n",
    "pprint(evaluation_result)\n",
    "print(f\"Final accuracy: {evaluation_result[-1][TEST_ACCURACY] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, almost as good as offline training. Why it works so well?\n",
    "\n",
    "The resaon behind it is that all the training batches have (in average) **data from the same domain**. So this incremental procedure is equal to classic (offline) training with train_epochs=1, because we we set TRAIN_EPOCHS hyperparameter which makes trainer to perform one pass over the data. So even we train the model incrementally, it is completely fine until all batches have similarly distributed data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (10 min)\n",
    "\n",
    "How **n_experiences** (number of experience) influences the final accuracy? Check different setups, especially:\n",
    "- 1\n",
    "- 10\n",
    "- and some large number close to train dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXPERIENCES = []  # Fill this list to check different number of experiences\n",
    "\n",
    "results = {}\n",
    "for n_experiences in N_EXPERIENCES:\n",
    "    print(f\"Training n_experiences={n_experiences}\")\n",
    "    evaluation_result = train_incremental(n_experiences=n_experiences, evaluator=NO_LOGGING_EVALUATOR)\n",
    "    results[n_experiences] = evaluation_result\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = [results[-1][TEST_ACCURACY] * 100 for key, results in results.items()]\n",
    "df_results = pd.DataFrame({'Number of experiences': [n_exp for n_exp in results.keys()], 'Accuracy [%]': metric_results})\n",
    "sns.barplot(data=df_results, x='Number of experiences', y='Accuracy [%]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does the number of experiences matter?\n",
    "- Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Incremental Continual Learning\n",
    "\n",
    "This is the medium difficult scenario in Continual Learning. The main difficulty is that **data distribution is changning over time**, e.g. we have to train o **MNIST** and then on the **RotatedMNIST** dataset. Classes are the same, but they have to be trained on the slightly different data over time.\n",
    "\n",
    "![CL-Domain-Incremental.png](https://drive.google.com/uc?export=view&id=1U6jfS3dTsEQ12BEocJw1KoibNh8sleGo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 1\n",
    "DROP_RATE = 0.5\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "N_EXPERIENCES = 2\n",
    "ROTATION_LIST = [60, 180]\n",
    "\n",
    "def train_domain_incremental(method) -> list[dict[str, float]]:\n",
    "    # Datasets\n",
    "    mnist = ni_benchmark(\n",
    "        mnist_train,\n",
    "        mnist_test,\n",
    "        n_experiences=len(ROTATION_LIST),\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "        balance_experiences=True,\n",
    "    )\n",
    "    mnist_train_stream = mnist.train_stream\n",
    "    mnist_test_stream = mnist.test_stream\n",
    "    \n",
    "    rotated_mnist = RotatedMNIST(\n",
    "        n_experiences=len(ROTATION_LIST),\n",
    "        rotations_list=ROTATION_LIST,\n",
    "        seed=SEED\n",
    "    )\n",
    "    rotated_mnist_train_stream = rotated_mnist.train_stream\n",
    "    rotated_mnist_test_stream = rotated_mnist.test_stream\n",
    "\n",
    "    # Train and test\n",
    "    results_mnist, results_rotated_mnist = [], []\n",
    "\n",
    "    # Train on MNIST and eval on both datasets\n",
    "    for experience in mnist_train_stream:\n",
    "        method.train(experience)\n",
    "    \n",
    "    results_mnist.append(method.eval(mnist_test_stream))\n",
    "    results_rotated_mnist.append(method.eval(rotated_mnist_test_stream))\n",
    "\n",
    "    # Train on RotatedMNIST and eval on both datasets\n",
    "    for experience in rotated_mnist_train_stream:\n",
    "        method.train(experience)\n",
    "    \n",
    "    results_mnist.append(method.eval(mnist_test_stream))\n",
    "    results_rotated_mnist.append(method.eval(rotated_mnist_test_stream))\n",
    "\n",
    "    return results_mnist, results_rotated_mnist\n",
    "\n",
    "\n",
    "def plot_domain_incremental(results_mnist: list[dict[str, float]], results_rotated_mnist: list[dict[str, float]]) -> None:\n",
    "    # Plot result\n",
    "    acc_mnist_1 = results_mnist[0][TEST_ACCURACY] * 100\n",
    "    acc_mnist_2 = results_mnist[1][TEST_ACCURACY] * 100\n",
    "    acc_rot_mnist_1 = results_rotated_mnist[0][TEST_ACCURACY] * 100\n",
    "    acc_rot_mnist_2 = results_rotated_mnist[1][TEST_ACCURACY] * 100\n",
    "    df_data = pd.DataFrame({\n",
    "        'Task (1 - training on MNIST, 2 - training on RotatedMNIST)': [1, 1, 2, 2],\n",
    "        'Accuracy [%]': [acc_mnist_1, acc_rot_mnist_1, acc_mnist_2, acc_rot_mnist_2],\n",
    "        'Dataset': ['MNIST', 'RotatedMNIST', 'MNIST', 'RotatedMNIST'],\n",
    "    })\n",
    "    \n",
    "    sns.barplot(data=df_data, x='Task (1 - training on MNIST, 2 - training on RotatedMNIST)', y='Accuracy [%]', hue='Dataset')\n",
    "    plt.show()\n",
    "    \n",
    "    avg_accuracy = (acc_mnist_2 + acc_rot_mnist_2) / 2\n",
    "    print()\n",
    "    print(f\"\\nFinal average accuracy: {avg_accuracy:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive\n",
    "\n",
    "Let's train with a standard, Naive approach and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Joint training strategy\n",
    "naive_train = Naive(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    ")\n",
    "\n",
    "results_mnist, results_rotated_mnist = train_domain_incremental(method=naive_train)\n",
    "plot_domain_incremental(results_mnist, results_rotated_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST has lost a lot of accuracy after training on the RotatedMNIST. This phenomenon is called a **Catastrophic Forgetting**.\n",
    "\n",
    "![Catastrophic-Forgetting.png](https://drive.google.com/uc?export=view&id=1EfzEk2zl2796fuWfAeaCJo5uqYBPeb-u)\n",
    "\n",
    "How can we prevent it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWC\n",
    "\n",
    "**E**lastic **W**eights **C**onsolidation is a regularization-based method inspired by advanced neuroscience memory consolidation theories.\n",
    "\n",
    "\n",
    "**Idea**: limiting the learning of parameters critical to the performance of past tasks, as\n",
    "measured by the Fisher Information Matrix.\n",
    "\n",
    "**Intuition**: Update parameters in such way that it is minimally invasive for old tasks and allows to acquire new knowledge\n",
    "\n",
    "![EWC.png](https://drive.google.com/uc?export=view&id=1sRhxupi8AXzVbFpJtdlXQ_CzsOMaHPiL) ![EWC-math.png](https://drive.google.com/uc?export=view&id=1-RueSm--zAZYQDZghatTPjVh0_kBwIjE)\n",
    "\n",
    "Kirkpatrick, James, et al. \"Overcoming catastrophic forgetting in neural networks.\" Proceedings of the national academy of sciences 114.13 (2017): 3521-3526."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "EWC_LAMBDA = 0.5\n",
    "\n",
    "# Model\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# EWC training strategy\n",
    "ewc = EWC(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    "    # EWC hyperparameters\n",
    "    ewc_lambda=EWC_LAMBDA,\n",
    ")\n",
    "\n",
    "results_mnist, results_rotated_mnist = train_domain_incremental(method=ewc)\n",
    "plot_domain_incremental(results_mnist, results_rotated_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEM\n",
    "\n",
    "**G**radien **E**pisodic **M**emory is a memory-based and regularization-based approach that tries to keep stable performance on already learned tasks. GEM solves a constrained optimization problem, gavoids increasing loss on old tasks. It projects the current task gradient in a feasible area, outlined by the previous task gradients and uses memory with random sampling. Thanks to this GEM's main target is to favour backward transfer and online learning capabilities.\n",
    "\n",
    "**Idea**: constrain new task updates to not interfere with previous tasks.\n",
    "\n",
    "**Intuition**: Update parameters in such way that it is minimally invasive for old tasks and allows to acquire new knowledge.\n",
    "\n",
    "![image.png](https://drive.google.com/uc?export=view&id=1jNJ4MdwS7bYlU5j6bAWlou3tNjrDLOjT)\n",
    "\n",
    "Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" Advances in neural information processing systems 30 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "PATTERNS_PER_EXP = 200\n",
    "\n",
    "# Model\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# GEM training strategy\n",
    "gem = GEM(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    "    # GEM hyperparameters\n",
    "    patterns_per_exp=PATTERNS_PER_EXP,\n",
    ")\n",
    "\n",
    "results_mnist, results_rotated_mnist = train_domain_incremental(method=gem)\n",
    "plot_domain_incremental(results_mnist, results_rotated_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 min)\n",
    "\n",
    "Can you improve EWC method performance? Try to search for better **ewc_lambda**.\n",
    "\n",
    "Tip: ewc_lambda is a hyperparameter controlling strength of regularization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EWC_LAMBDAS = [0.5]  # Fill this list with three/four different values to check ewc_lambda setups\n",
    "\n",
    "accuracies = {}\n",
    "for ewc_lambda in EWC_LAMBDAS:\n",
    "    print(f\"Training ewc_lambda={ewc_lambda}\")\n",
    "    \n",
    "    # Model\n",
    "    # To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "    model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "    \n",
    "    # Prepare for training & testing\n",
    "    optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    # EWC training strategy\n",
    "    ewc = EWC(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        train_epochs=TRAIN_EPOCHS,\n",
    "        train_mb_size=BATCH_SIZE_TRAIN,\n",
    "        eval_mb_size=BATCH_SIZE_TEST,\n",
    "        device=DEVICE,\n",
    "        evaluator=NO_LOGGING_EVALUATOR,\n",
    "        # EWC hyperparameters\n",
    "        ewc_lambda=ewc_lambda,\n",
    "    )\n",
    "\n",
    "    results_mnist, results_rotated_mnist = train_domain_incremental(method=ewc)\n",
    "    accuracies[ewc_lambda] = {'MNIST accuracy': results_mnist[-1][TEST_ACCURACY], 'Rotated MNIST accuracy': results_rotated_mnist[-1][TEST_ACCURACY]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({'EWC Lambda': list(accuracies.keys()), 'Average Accuracy [%]': [sum(value.values()) / len(value) for value in accuracies.values()]})\n",
    "sns.barplot(data=df_data, x='EWC Lambda', y='Average Accuracy [%]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the good choice for **ewc_lambda**?\n",
    "- Can you explain why?\n",
    "- How it affects the result?\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Incremental Continual Learning\n",
    "\n",
    "This is the most difficult scenario in Continual Learning. We will work with the most challenging setup where each class is presented separately. So firstly the model see only the data labeled as 0, then 1, and so on..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CL-Class-Incremental.png](https://drive.google.com/uc?export=view&id=1Y7Mo4YIuJNSnhEV7i_Lnzprkp-SU82mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 1\n",
    "DROP_RATE = 0.5\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "FIXED_CLASS_ORDER = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "N_EXPERIENCES = 10\n",
    "\n",
    "\n",
    "def train_class_incremental(method) -> list[dict[str, float]]:\n",
    "    # Dataset\n",
    "    mnist = SplitMNIST(n_experiences=N_EXPERIENCES, fixed_class_order=FIXED_CLASS_ORDER)\n",
    "    train_stream = mnist.train_stream\n",
    "    test_stream = mnist.test_stream\n",
    "    \n",
    "    # Train and test\n",
    "    results = []\n",
    "    method.train(train_stream)\n",
    "    evaluation_result = method.eval(test_stream)\n",
    "    results.append(evaluation_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Joint training strategy\n",
    "joint_train = Naive(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    ")\n",
    "\n",
    "naive_results = train_class_incremental(method=joint_train)\n",
    "print(f\"Final test accuracy: {naive_results[-1][TEST_ACCURACY] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty bad Accuracy, isn't is? Final Accurracy looks like a random guessing.\n",
    "\n",
    "This phenomenon is called a Recency Bias [5]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "EWC_LAMBDA = 0.5\n",
    "\n",
    "# Model.\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# EWC training strategy\n",
    "ewc = EWC(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    "    # EWC hyperparameters\n",
    "    ewc_lambda=EWC_LAMBDA,\n",
    ")\n",
    "\n",
    "ewc_results = train_class_incremental(method=ewc)\n",
    "print(f\"Final test accuracy: {ewc_results[-1][TEST_ACCURACY] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "PATTERNS_PER_EXP = 20\n",
    "\n",
    "# Model.\n",
    "# To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# GEM training strategy\n",
    "gem = GEM(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_epochs=TRAIN_EPOCHS,\n",
    "    train_mb_size=BATCH_SIZE_TRAIN,\n",
    "    eval_mb_size=BATCH_SIZE_TEST,\n",
    "    device=DEVICE,\n",
    "    evaluator=NO_LOGGING_EVALUATOR,\n",
    "    # GEM hyperparameters\n",
    "    patterns_per_exp=PATTERNS_PER_EXP,\n",
    ")\n",
    "\n",
    "gem_results = train_class_incremental(method=gem)\n",
    "print(f\"Final test accuracy: {gem_results[-1][TEST_ACCURACY] * 100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual Learning Methods Comparison\n",
    "\n",
    "Let's compare Naive, EWC and GEM methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_incremental_results = [naive_results, ewc_results, gem_results]\n",
    "df_results = pd.DataFrame({\n",
    "    'Method': [\"Naive\", \"EWC (regularization)\", \"GEM (memory)\"],\n",
    "    'Accuracy [%]': [result[-1][TEST_ACCURACY] * 100 for result in class_incremental_results]\n",
    "})\n",
    "sns.barplot(data=df_results, x='Method', y='Accuracy [%]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-based method achieved the best performance and outperformed other methods by the significant margin. Why such thing happened? Class Incremental Continual Learning is the most difficult scenario and there is extremely hard to train a model when data from each class is presented separately. **Neural networks are overly plastic** and gradient-based optimization algorithms are handling such situation poorly.\n",
    "\n",
    "The most common solution for Class Incremental Continual Learning problems is to keep some examples in the memory. It's very simple but brutally effective. Unfortunately, we not always have possibility to store examples in the memory. In lot of real-world applications data cannot be stored, e.g. due to RODO or privacy purposed or should be anonymyzed which makes it's useless for additional model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (10 min)\n",
    "\n",
    "Compare GEM model performance with different memory buffer sizes. Try to determine what impact has memory buffer size on model accuracy. The more the better? Or there is other pattern? Fill the PATTERNS_PER_EXPERIENCE list below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATTERNS_PER_EXPERIENCE = []  # Fill that list to try four/five different memory sizes and run the cell\n",
    "\n",
    "evaluation_results = {}\n",
    "for patterns_per_exp in PATTERNS_PER_EXPERIENCE:\n",
    "    print(f\"Training patterns_per_exp={patterns_per_exp}\")\n",
    "    \n",
    "    # Model.\n",
    "    # To make experiments fair, remember to always initialize a new model and not to use already trained one.\n",
    "    model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "    \n",
    "    # Prepare for training & testing\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    # GEM training strategy\n",
    "    gem = GEM(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        train_epochs=TRAIN_EPOCHS,\n",
    "        train_mb_size=BATCH_SIZE_TRAIN,\n",
    "        eval_mb_size=BATCH_SIZE_TEST,\n",
    "        device=DEVICE,\n",
    "        evaluator=NO_LOGGING_EVALUATOR,\n",
    "        # GEM hyperparameters\n",
    "        patterns_per_exp=patterns_per_exp,\n",
    "    )\n",
    "    \n",
    "    evaluation_result = train_class_incremental(method=gem)\n",
    "    evaluation_results[patterns_per_exp] = evaluation_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the results when experiments are finished. Just run the cell below. Remember that the hyperparameter that we experiment with is \"patterns_per_exp\". Experience = class in our case, so the total number of examples in the memory is number_of_classes times patterns_per_exp (10 * patterns_per_exp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_gem_results = [results[-1][TEST_ACCURACY] * 100 for key, results in evaluation_results.items()]\n",
    "df_results = pd.DataFrame({'Memory size': [mem_size * 10 for mem_size in evaluation_results.keys()], 'Accuracy [%]': memory_gem_results})\n",
    "sns.barplot(data=df_results, x='Memory size', y='Accuracy [%]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What was the best memory size? (memory size related to the highest accuracy)\n",
    "- How memory size impact accuracy?\n",
    "- What is a good trade-off between memory buffer size and accuracy? Can you find it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (10 min)\n",
    "\n",
    "We know that memory has a crucial role in Class Incremental Continual Learning, but we not always can store as many examples as we could. In this excercise your task is to determine the minimum number of examples needed to achieve accuracy >= 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZES = [10]  # Fill that list to try four/five different memory sizes and run the cell\n",
    "\n",
    "evaluation_results = {}\n",
    "for memory_size in MEMORY_SIZES:\n",
    "    print(f\"Train memory_size={memory_size}\")\n",
    "    \n",
    "    model = MLP(input_layer_size=INPUT_LAYER_SIZE, hidden_layer_size=HIDDEN_LAYER_SIZE, output_layer_size=OUTPUT_LAYER_SIZE, drop_rate=DROP_RATE)\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    replay = Replay(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        mem_size=memory_size,\n",
    "        train_epochs=TRAIN_EPOCHS,\n",
    "        train_mb_size=BATCH_SIZE_TRAIN,\n",
    "        eval_mb_size=BATCH_SIZE_TEST,\n",
    "        device=DEVICE,\n",
    "        evaluator=NO_LOGGING_EVALUATOR,\n",
    "    )\n",
    "    \n",
    "    evaluation_result = train_class_incremental(method=replay)\n",
    "    evaluation_results[memory_size] = evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = [results[-1][TEST_ACCURACY] * 100 for key, results in evaluation_results.items()]\n",
    "df_results = pd.DataFrame({'Memory size': [mem_size for mem_size in evaluation_results.keys()], 'Accuracy [%]': all_results})\n",
    "\n",
    "if not df_results.empty:\n",
    "    sns.barplot(data=df_results, x='Memory size', y='Accuracy [%]')\n",
    "\n",
    "plt.axhline(70, linestyle='--', color='black')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What memory size allows to achieve accuracy >= 70%?\n",
    "- Is's a lot or a little?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final assingment\n",
    "\n",
    "In this assingment your task will be to effectively train a model on MNIST and Permuted MNIST datasets. We already know something about the Continual Learning and it's scenarios and we will try to use this knowledge in practice, so don't hesitate to reuse a code from any place of this notebook. Preferably, the solution should:\n",
    "- make changes in the model (may be architecture or just using different model. e.g. [SimpleCNN](https://avalanche.continualai.org/from-zero-to-hero-tutorial/02_models))\n",
    "- use a new Continual Learning method... (see [methods](https://avalanche-api.continualai.org/en/v0.3.1/training.html#training-strategies))\n",
    "- ... or use combination of already known methods (see [example](https://avalanche.continualai.org/from-zero-to-hero-tutorial/04_training#adding-plugins))\n",
    "- add some new metric to evaluation (see [metrics](https://avalanche-api.continualai.org/en/v0.3.1/evaluation.html))\n",
    "- have reasonable accuracy (at least > **90%** after training on both tasks)\n",
    "- (optionally) perform HP search\n",
    "- (optionally) experiment with a loss function, optimizer, number of epochs on each and other hyperparameters\n",
    "- (optionally) what is the best result that you were able to achieve without a memory?\n",
    "\n",
    "We had prepared a experimental template for you, so you can complete the assingment by filling the commented sections with your code. The purpose of this assingment is experiment with various settings and learn by trying (and failing). Write down your observations and conculsions.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - try with your own\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_EPOCHS = 1\n",
    "DROP_RATE = 0.5\n",
    "INPUT_LAYER_SIZE = 784\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "N_EXPERIENCES_MNIST = 2  # On how many data portions will be divided MNIST training\n",
    "N_EXPERIENCES_PERMUTED_MNIST = 2  # On how many data portions will be divided PermutedMNIST training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trainig & Evaluation code. Do not modify this function ---\n",
    "\n",
    "def train_mnist_and_permuted_mnist(method) -> tuple[list[dict[str, float]], list[dict[str, float]]]:\n",
    "    # Datasets\n",
    "    mnist = ni_benchmark(\n",
    "        mnist_train,\n",
    "        mnist_test,\n",
    "        n_experiences=N_EXPERIENCES_MNIST,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "        balance_experiences=True,\n",
    "    )\n",
    "    mnist_train_stream = mnist.train_stream\n",
    "    mnist_test_stream = mnist.test_stream\n",
    "    \n",
    "    permuted_mnist = PermutedMNIST(\n",
    "        n_experiences=N_EXPERIENCES_PERMUTED_MNIST,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    permuted_mnist_train_stream = permuted_mnist.train_stream\n",
    "    permuted_mnist_test_stream = permuted_mnist.test_stream\n",
    "    \n",
    "    # Train and test\n",
    "    results_mnist, results_permuted_mnist = [], []\n",
    "    \n",
    "    # Train on MNIST and eval on both datasets\n",
    "    method.train(mnist_train_stream)\n",
    "    \n",
    "    results_mnist.append(method.eval(mnist_test_stream))\n",
    "    results_permuted_mnist.append(method.eval(permuted_mnist_test_stream))\n",
    "    \n",
    "    # Train on PermutedMNIST and eval on both datasets\n",
    "    method.train(permuted_mnist_train_stream)\n",
    "    \n",
    "    results_mnist.append(method.eval(mnist_test_stream))\n",
    "    results_permuted_mnist.append(method.eval(permuted_mnist_test_stream))\n",
    "\n",
    "    return results_mnist, results_permuted_mnist\n",
    "\n",
    "# --- Trainig & Evaluation code end ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Your code starts here --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your model\n",
    "\n",
    "# Prepare yout optimizer\n",
    "\n",
    "# Prepare your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your method\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Your code ends here ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Evaluate\n",
    "\n",
    "results_mnist, results_permuted_mnist = train_mnist_and_permuted_mnist(method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "\n",
    "acc_mnist_1 = results_mnist[0][TEST_ACCURACY] * 100\n",
    "acc_mnist_2 = results_mnist[1][TEST_ACCURACY] * 100\n",
    "acc_perm_mnist_1 = results_permuted_mnist[0][TEST_ACCURACY] * 100\n",
    "acc_perm_mnist_2 = results_permuted_mnist[1][TEST_ACCURACY] * 100\n",
    "df_data = pd.DataFrame({\n",
    "    'Task (1 - training on MNIST, 2 - training on PermutedMNIST)': [1, 1, 2, 2],\n",
    "    'Accuracy [%]': [acc_mnist_1, acc_perm_mnist_1, acc_mnist_2, acc_perm_mnist_2],\n",
    "    'Dataset': ['MNIST', 'PermutedMNIST', 'MNIST', 'PermutedMNIST'],\n",
    "})\n",
    "\n",
    "sns.barplot(data=df_data, x='Task (1 - training on MNIST, 2 - training on PermutedMNIST)', y='Accuracy [%]', hue='Dataset')\n",
    "plt.axhline(90, linestyle='--', color='black')\n",
    "plt.show()\n",
    "\n",
    "avg_accuracy = (acc_mnist_2 + acc_perm_mnist_2) / 2\n",
    "message = \"great, you have accomplished the assingment!\" if avg_accuracy >= 90 else \"try to improve a litte bit.\"\n",
    "\n",
    "print()\n",
    "print(f\"\\nFinal average accuracy: {avg_accuracy:.2f}% - {message}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "\n",
    "- [Three scenarios for continual learning - obligatory lecture](https://arxiv.org/abs/1904.07734)\n",
    "- [How architecture affects the Continual Learning performance?](https://arxiv.org/abs/2202.00275)\n",
    "- [LFPT5 - Continual Learning approach for generative LLMs](https://arxiv.org/pdf/2110.07298.pdf)\n",
    "- [S-Prompts - a solution for difficult problems where You cannot have a memory buffer](https://arxiv.org/pdf/2207.12819.pdf)\n",
    "- [Survey for try-hards](https://arxiv.org/abs/2302.00487)\n",
    "- [Avalanche site](https://avalanche.continualai.org/)\n",
    "- [Avalanche Github](https://github.com/ContinualAI/avalanche)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] De Lange, Matthias, et al. \"A continual learning survey: Defying forgetting in classification tasks.\" IEEE transactions on pattern analysis and machine intelligence 44.7 (2021): 3366-3385.  \n",
    "[2] Lomonaco, Vincenzo, et al. \"Avalanche: an end-to-end library for continual learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.  \n",
    "[3] Kirkpatrick, James, et al. \"Overcoming catastrophic forgetting in neural networks.\" Proceedings of the national academy of sciences 114.13 (2017): 3521-3526.  \n",
    "[4] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" Advances in neural information processing systems 30 (2017).  \n",
    "[5] Zheda Mai, Ruiwen Li, Hyunwoo Kim, Scott Sanner. \"Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
